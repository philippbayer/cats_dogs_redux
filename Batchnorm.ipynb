{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "\n",
    "#path = \"data/dogscats/sample/\"\n",
    "path = \"data/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "\n",
    "batch_size=64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg_ft(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21000 images belonging to 2 classes.\n",
      "Found 4000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_array(model_path + 'train_convlayer_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_convlayer_features.bc', val_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_convlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the weights from the pre-trained model.\n",
    "# NB: Since we're removing dropout, we want to half the weights\n",
    "def proc_wgts(layer): return [o/2 for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Such a finely tuned model needs to be updated very slowly!\n",
    "opt = RMSprop(lr=0.00001, rho=0.7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model():\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21000 images belonging to 2 classes.\n",
      "Found 4000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "\n",
    "\n",
    "\n",
    "batches = get_batches(path+'train', gen, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "fc_model = get_fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0421 - acc: 0.9860 - val_loss: 0.0617 - val_acc: 0.9840\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0101 - acc: 0.9973 - val_loss: 0.0679 - val_acc: 0.9850\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1116 - val_acc: 0.9822\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1271 - val_acc: 0.9845\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1279 - val_acc: 0.9830\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1370 - val_acc: 0.9832\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1310 - val_acc: 0.9828\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 31s - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1359 - val_acc: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa7b762650>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(trn_features, trn_labels, nb_epoch=8, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.save_weights(model_path+'no_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = get_fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in conv_model.layers: layer.trainable = False\n",
    "# Look how easy it is to connect two models together!\n",
    "conv_model.add(fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 623s - loss: 0.0533 - acc: 0.9816 - val_loss: 0.0871 - val_acc: 0.9732\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 624s - loss: 0.0414 - acc: 0.9870 - val_loss: 0.0618 - val_acc: 0.9828\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 622s - loss: 0.0345 - acc: 0.9894 - val_loss: 0.0642 - val_acc: 0.9832\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 624s - loss: 0.0272 - acc: 0.9906 - val_loss: 0.0634 - val_acc: 0.9840\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 625s - loss: 0.0279 - acc: 0.9921 - val_loss: 0.0659 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 628s - loss: 0.0223 - acc: 0.9934 - val_loss: 0.0793 - val_acc: 0.9810\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 628s - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0782 - val_acc: 0.9835\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 629s - loss: 0.0177 - acc: 0.9952 - val_loss: 0.0751 - val_acc: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa8addfa50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "conv_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=8,                        \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(model_path + 'aug1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(1000, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_wgts(layer, prev_p, new_p):\n",
    "    scal = (1-prev_p)/(1-new_p)\n",
    "    return [o*scal for o in layer.get_weights()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bn_model.pop()\n",
    "for layer in bn_model.layers: layer.trainable=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.3203 - acc: 0.8682 - val_loss: 0.2359 - val_acc: 0.9028\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.2763 - acc: 0.8977 - val_loss: 0.3251 - val_acc: 0.8815\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.2915 - acc: 0.8987 - val_loss: 0.2332 - val_acc: 0.9107\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.2982 - acc: 0.8980 - val_loss: 0.2119 - val_acc: 0.9165\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.2963 - acc: 0.9003 - val_loss: 0.2341 - val_acc: 0.9113\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.3100 - acc: 0.8978 - val_loss: 0.2610 - val_acc: 0.9077\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.3198 - acc: 0.8950 - val_loss: 0.2179 - val_acc: 0.9165\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 13s - loss: 0.3107 - acc: 0.8969 - val_loss: 0.2180 - val_acc: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa72e3cad0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(trn_features, trn_labels, nb_epoch=8, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(model_path+'bn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_layers = get_bn_layers(0.6)\n",
    "bn_layers.pop()\n",
    "bn_layers.append(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_model = Sequential(conv_layers)\n",
    "for layer in final_model.layers: layer.trainable = False\n",
    "for layer in bn_layers: final_model.add(layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(bn_model.layers, bn_layers):\n",
    "    l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(optimizer=Adam(), \n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "21000/21000 [==============================] - 632s - loss: 0.3065 - acc: 0.9530 - val_loss: 0.1097 - val_acc: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa66444810>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=1, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.save_weights(model_path + 'final1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 635s - loss: 0.1141 - acc: 0.9656 - val_loss: 0.0657 - val_acc: 0.9795\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 637s - loss: 0.0828 - acc: 0.9718 - val_loss: 0.0779 - val_acc: 0.9762\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 637s - loss: 0.0633 - acc: 0.9776 - val_loss: 0.0632 - val_acc: 0.9822\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 636s - loss: 0.0644 - acc: 0.9774 - val_loss: 0.0521 - val_acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa65d9e950>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=4, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "test_batches = get_batches(test_path, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "conv_test_feat = final_model.predict_generator(test_batches, test_batches.nb_sample)\n",
    "\n",
    "save_array('conv_test_feat.dat', conv_test_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preds = bn_model.predict(conv_test_feat, batch_size=batch_size*2)\n",
    "#preds\n",
    "isdog = conv_test_feat[:,1]\n",
    "#isdog = preds[:,1]\n",
    "isdog = isdog.clip(min=0.00000001, max=0.99999999)\n",
    "filenames = test_batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])\n",
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm\n",
    "submission_file_name = 'submission2.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
